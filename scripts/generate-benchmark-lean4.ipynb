{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a42dda6",
   "metadata": {},
   "source": [
    "Constructing LeanDojo Benchmark (Lean 4)\n",
    "===================================\n",
    "\n",
    "This script uses [LeanDojo](https://leandojo.org/) to construct LeanDojo Benchmark 4 in the appendix of our paper:\n",
    "\n",
    "[LeanDojo: Theorem Proving with Retrieval-Augmented Language Models](https://leandojo.org/)      \n",
    "Under review at NeurIPS (Datasets and Benchmarks Track), 2023  \n",
    "[Kaiyu Yang](https://yangky11.github.io/), [Aidan Swope](https://aidanswope.com/about), [Alex Gu](https://minimario.github.io/), [Rahul Chalamala](https://rchalamala.github.io/), [Peiyang Song](https://www.linkedin.com/in/peiyang-song-3279b3251/), [Shixing Yu](https://billysx.github.io/), [Saad Godil](https://www.linkedin.com/in/saad-godil-9728353/), [Ryan Prenger](https://www.linkedin.com/in/ryan-prenger-18797ba1/), [Anima Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/)\n",
    "\n",
    "The dataset is constructed from [mathlib4](https://github.com/leanprover-community/mathlib4/tree/5a919533f110b7d76410134a237ee374f24eaaad) (`5a919533f110b7d76410134a237ee374f24eaaad`) and will be saved to `../leandojo_benchmark_4`. It includes 2000 theorems for validation, 2000 theorems for testing, and the rest for training. Please refer to our paper for details. For most use cases, you shouldn't need to generate the data and can directly use our official LeanDojo Benchmark 4 downloadable [here](https://zenodo.org/record/8040110).\n",
    "\n",
    "This script is for Lean 4. We also have a more detailed [version for Lean3](https://github.com/lean-dojo/LeanDojo/blob/main/scripts/generate-benchmark-lean3.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5710e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import shutil\n",
    "# import random\n",
    "# from copy import copy\n",
    "# from pathlib import Path\n",
    "# from loguru import logger\n",
    "# from datetime import datetime\n",
    "# from typing import Dict, List, Union\n",
    "\n",
    "import ray\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from ray.util.actor_pool import ActorPool\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import lean_dojo\n",
    "from lean_dojo import *\n",
    "\n",
    "random.seed(3407)  # https://arxiv.org/abs/2109.08203\n",
    "\n",
    "URL = \"https://github.com/leanprover-community/mathlib4\"\n",
    "COMMIT = \"5a919533f110b7d76410134a237ee374f24eaaad\"\n",
    "DST_DIR = Path(\"../leandojo_benchmark_4\")\n",
    "NUM_VAL = NUM_TEST = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b666e",
   "metadata": {},
   "source": [
    "## Splitting the Theorems\n",
    "\n",
    "We will split the theorems into train/val/test using two different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a34ccdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_NAME = str  # train/val/test\n",
    "SPLIT = Dict[SPLIT_NAME, List[TracedTheorem]]\n",
    "SPLIT_STRATEGY = str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e099d",
   "metadata": {},
   "source": [
    "### Splitting Randomly\n",
    "\n",
    "The first and the simplest strategy is splitting the theorems randomly, which can be implemented by a random shuffle followed by a sequential split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1beb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_sequentially(\n",
    "    traced_theorems: List[TracedTheorem],\n",
    ") -> SPLIT:\n",
    "    \"\"\"Split ``traced_theorems`` sequentially into train/val/test.\"\"\"\n",
    "    num_theorems = len(traced_theorems)\n",
    "    num_train = num_theorems - NUM_VAL - NUM_TEST\n",
    "    return {\n",
    "        \"train\": traced_theorems[:num_train],\n",
    "        \"val\": traced_theorems[num_train : num_train + NUM_VAL],\n",
    "        \"test\": traced_theorems[num_train + NUM_VAL :],\n",
    "    }\n",
    "\n",
    "\n",
    "def split_randomly(\n",
    "    traced_theorems: List[TracedTheorem],\n",
    ") -> SPLIT:\n",
    "    \"\"\"Split ``traced_theorems`` randomly into train/val/test.\"\"\"\n",
    "    logger.info(\"Spliting the theorems randomly\")\n",
    "    traced_theorems = copy(traced_theorems)\n",
    "    random.shuffle(traced_theorems)\n",
    "    return _split_sequentially(traced_theorems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65fe11",
   "metadata": {},
   "source": [
    "### Splitting by Premise\n",
    "\n",
    "The second strategy is splitting by premise. We want to test the prover's capability in using novel premises, i.e., premises that have never been used in training. Please see the implementation below. Note that validation and testing theorems may share premises. So the testing performance should be reported using models trained on the training set only, NOT training plus validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e150f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_premise(\n",
    "    traced_theorems: List[TracedTheorem],\n",
    ") -> SPLIT:\n",
    "    \"\"\"\n",
    "    Split theorems into train/val/test so that proofs in val/test rely on at\n",
    "    least one novel premise that does not appear in train.\n",
    "    \"\"\"\n",
    "    logger.info(\"Spliting the theorems by premises\")\n",
    "\n",
    "    # Figure out the number of theorems in train/val/test.\n",
    "    num_theorems = len(traced_theorems)\n",
    "    num_val_test = NUM_VAL + NUM_TEST\n",
    "    num_train = num_theorems - num_val_test\n",
    "    theorems_val_test = set()\n",
    "\n",
    "    # Map each premise to a list of theorems using it.\n",
    "    theorems_by_premises = defaultdict(list)\n",
    "    for t in traced_theorems:\n",
    "        for p in t.get_premise_full_names():\n",
    "            theorems_by_premises[p].append(t)\n",
    "\n",
    "    # Sort the premises by the number of theorems using them (in ascending order).\n",
    "    theorems_by_premises = sorted(theorems_by_premises.items(), key=lambda x: len(x[1]))\n",
    "\n",
    "    # For each premise, put all theorems using it into val_test so that it does not appear in train.\n",
    "    for _, thms in theorems_by_premises:\n",
    "        if len(theorems_val_test) < num_val_test:\n",
    "            theorems_val_test.update(thms)\n",
    "\n",
    "    # All other theorems go to train.\n",
    "    theorems_train = [t for t in traced_theorems if t not in theorems_val_test]\n",
    "    theorems_val_test = list(theorems_val_test)\n",
    "    random.shuffle(theorems_val_test)\n",
    "\n",
    "    return {\n",
    "        \"train\": theorems_train,\n",
    "        \"val\": theorems_val_test[:NUM_VAL],\n",
    "        \"test\": theorems_val_test[NUM_VAL:],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cbecd6",
   "metadata": {},
   "source": [
    "Given a traced repo, we can split the theorems using these strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03882cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(traced_repo: TracedRepo) -> Dict[SPLIT_STRATEGY, SPLIT]:\n",
    "    traced_theorems = traced_repo.get_traced_theorems()\n",
    "    logger.info(f\"{len(traced_theorems)} theorems in total\")\n",
    "\n",
    "    return {\n",
    "        \"random\": split_randomly(traced_theorems),\n",
    "        \"novel_premises\": split_by_premise(traced_theorems),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5b015",
   "metadata": {},
   "source": [
    "## Exporting the Data\n",
    "Once theorems are splitted into train/val/test. We export them to JSON formats that can be easily used in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06e6fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(\n",
    "    traced_repo: TracedRepo,\n",
    "    splits: Dict[SPLIT_STRATEGY, SPLIT],\n",
    "    dst_path: Union[str, Path],\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    \"\"\"Export a traced repo whose theorems have been splitted to ``dst_path``.\"\"\"\n",
    "    if isinstance(dst_path, str):\n",
    "        dst_path = Path(dst_path)\n",
    "    if dst_path.exists():\n",
    "        logger.warning(f\"{dst_path} already exists. Removing it now.\")\n",
    "        shutil.rmtree(dst_path)\n",
    "\n",
    "    # Export the proofs.\n",
    "    for strategy, split in splits.items():\n",
    "        split_dir = dst_path / strategy\n",
    "        split_dir.mkdir(parents=True)\n",
    "        for name, theorems in split.items():\n",
    "            data = []\n",
    "            num_tactics = 0\n",
    "            for thm in theorems:\n",
    "                tactics = [\n",
    "                    {\n",
    "                        \"tactic\": t.tactic,\n",
    "                        # \"annotated_tactic\": t.get_annotated_tactic(),\n",
    "                        \"state_before\": t.state_before,\n",
    "                        \"state_after\": t.state_after,\n",
    "                    }\n",
    "                    for t in thm.get_traced_tactics()\n",
    "                    if t.state_before != \"no goals\"\n",
    "                    and \"·\" not in t.tactic  # Ignore \"·\".\n",
    "                ]\n",
    "                num_tactics += len(tactics)\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"url\": thm.repo.url,\n",
    "                        \"commit\": thm.repo.commit,\n",
    "                        \"file_path\": str(thm.theorem.file_path),\n",
    "                        \"full_name\": thm.theorem.full_name,\n",
    "                        \"start\": list(thm.start),\n",
    "                        \"end\": list(thm.end),\n",
    "                        \"traced_tactics\": tactics,\n",
    "                    }\n",
    "                )\n",
    "            oup_path = split_dir / f\"{name}.json\"\n",
    "            json.dump(data, oup_path.open(\"wt\"))\n",
    "            logger.info(\n",
    "                f\"{len(theorems)} theorems and {num_tactics} tactics saved to {oup_path}\"\n",
    "            )\n",
    "            \n",
    "    # Export the premises (theorems, definitions, etc.).\n",
    "    oup_path = dst_path / \"corpus.jsonl\"\n",
    "    num_premises = 0\n",
    "    with oup_path.open(\"wt\") as oup:\n",
    "        G = traced_repo.traced_files_graph\n",
    "        for tf_node in reversed(list(nx.topological_sort(G))):\n",
    "            tf = G.nodes[tf_node][\"traced_file\"]\n",
    "            imports = [str(_) for _ in G.successors(tf_node)]\n",
    "            premises = tf.get_premise_definitions()\n",
    "            num_premises += len(premises)\n",
    "            oup.write(\n",
    "                json.dumps(\n",
    "                    {\"path\": str(tf.path), \"imports\": imports, \"premises\": premises}\n",
    "                )\n",
    "                + \"\\n\"\n",
    "            )\n",
    "    logger.info(\n",
    "        f\"{num_premises} theorems/definitions from {traced_repo.num_traced_files} files saved to {oup_path}\"\n",
    "    )\n",
    "\n",
    "    # Export the licenses.\n",
    "    license_dir = dst_path / \"licenses\"\n",
    "    license_dir.mkdir()\n",
    "    all_repos = [traced_repo.repo] + list(traced_repo.dependencies.values())\n",
    "    for repo in all_repos:\n",
    "        lic = repo.get_license()\n",
    "        if lic is None:\n",
    "            continue\n",
    "        with (license_dir / repo.name).open(\"wt\") as oup:\n",
    "            oup.write(lic)\n",
    "    with (license_dir / \"README.md\").open(\"wt\") as oup:\n",
    "        oup.write(\n",
    "            \"This directory contains licenses of Lean repos used to generate this dataset. The dataset itself is released under [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/).\"\n",
    "        )\n",
    "\n",
    "    # Export metadata.\n",
    "    metadata = dict(kwargs)\n",
    "    metadata[\"creation_time\"] = str(datetime.now())\n",
    "    metadata[\"from_repo\"] = {\n",
    "        \"url\": traced_repo.repo.url,\n",
    "        \"commit\": traced_repo.repo.commit,\n",
    "    }\n",
    "    metadata[\"leandojo_version\"] = lean_dojo.__version__\n",
    "    json.dump(metadata, (dst_path / \"metadata.json\").open(\"wt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc50220e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-12 13:19:46.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlean_dojo.data_extraction.trace\u001b[0m:\u001b[36mtrace\u001b[0m:\u001b[36m164\u001b[0m - \u001b[1mLoading the traced repo from /home/peiyang/.cache/lean_dojo/leanprover-community-mathlib4-5a919533f110b7d76410134a237ee374f24eaaad/mathlib4\u001b[0m\n",
      "2023-07-12 13:19:51,090\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3782/3782 [06:48<00:00,  9.27it/s]\n",
      "\u001b[32m2023-07-12 13:30:37.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_data\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1m91766 theorems in total\u001b[0m\n",
      "\u001b[32m2023-07-12 13:30:37.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_randomly\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mSpliting the theorems randomly\u001b[0m\n",
      "\u001b[32m2023-07-12 13:30:37.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_by_premise\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mSpliting the theorems by premises\u001b[0m\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m repo \u001b[38;5;241m=\u001b[39m LeanGitRepo(URL, COMMIT)\n\u001b[1;32m      2\u001b[0m traced_repo \u001b[38;5;241m=\u001b[39m trace(repo)\n\u001b[0;32m----> 3\u001b[0m splits \u001b[38;5;241m=\u001b[39m \u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraced_repo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m export_data(traced_repo, splits, DST_DIR, dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeanDojo Benchmark 4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m, in \u001b[0;36msplit_data\u001b[0;34m(traced_repo)\u001b[0m\n\u001b[1;32m      2\u001b[0m traced_theorems \u001b[38;5;241m=\u001b[39m traced_repo\u001b[38;5;241m.\u001b[39mget_traced_theorems()\n\u001b[1;32m      3\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(traced_theorems)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m theorems in total\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m: split_randomly(traced_theorems),\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnovel_premises\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43msplit_by_premise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraced_theorems\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      8\u001b[0m }\n",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m, in \u001b[0;36msplit_by_premise\u001b[0;34m(traced_theorems)\u001b[0m\n\u001b[1;32m     17\u001b[0m theorems_by_premises \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m traced_theorems:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_premise_full_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     20\u001b[0m         theorems_by_premises[p]\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Sort the premises by the number of theorems using them (in ascending order).\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/lean_dojo/data_extraction/traced_data.py:411\u001b[0m, in \u001b[0;36mTracedTheorem.get_premise_full_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the fully qualified names of all premises used in the proof.\"\"\"\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muses_lean4:\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    413\u001b[0m names \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_callback\u001b[39m(node: IdentNode, parents: List[Node]):\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "repo = LeanGitRepo(URL, COMMIT)\n",
    "traced_repo = trace(repo)\n",
    "splits = split_data(traced_repo)\n",
    "export_data(traced_repo, splits, DST_DIR, dataset_name=\"LeanDojo Benchmark 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492ea9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427982d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
